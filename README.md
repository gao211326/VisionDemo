
>å¤§é“å¦‚é’å¤©ï¼Œæˆ‘ç‹¬ä¸å¾—å‡º

#####å‰è¨€
åœ¨ä¸Šä¸€ç¯‡[iOS Core MLä¸Visionåˆè¯†](http://www.jianshu.com/p/b0e5f2944b3d)ä¸­ï¼Œåˆæ­¥äº†è§£åˆ°äº†`vision`çš„ä½œç”¨ï¼Œå¹¶åœ¨æ–‡ç« æœ€åç•™äº†ä¸ªç–‘é—®ï¼Œå°±æ˜¯ç±»ä¼¼ä¸‹é¢çš„ä¸€äº›å‡½æ•°æœ‰ä»€ä¹ˆç”¨
```
- (instancetype)initWithCIImage:(CIImage *)image options:(NSDictionary<VNImageOption, id> *)options;

- (instancetype)initWithCVPixelBuffer:(CVPixelBufferRef)pixelBuffer options:(NSDictionary<VNImageOption, id> *)options;
```
åœ¨æŸ¥é˜…ä¸€äº›èµ„æ–™åï¼Œæœ€ç»ˆé€šè¿‡è¿™äº›å‡½æ•°å¾—åˆ°äº†å¦‚ä¸‹çš„æ•ˆæœ

![face.gif](http://upload-images.jianshu.io/upload_images/2525768-c45ee7c16299c64a.gif?imageMogr2/auto-orient/strip)
å¯¹ï¼Œæ²¡é”™ï¼Œè¿™å°±æ˜¯é€šè¿‡`initWithCVPixelBuffer`å‡½æ•°æ¥å®ç°çš„ã€‚å½“ç„¶`vision`çš„ä½œç”¨è¿œä¸äºæ­¤ï¼Œè¿˜æœ‰å¦‚ä¸‹çš„æ•ˆæœ
1ã€å›¾åƒåŒ¹é…ï¼ˆä¸Šç¯‡æ–‡ç« ä¸­çš„æ•ˆæœï¼‰
2ã€çŸ©å½¢æ£€æµ‹
3ã€äºŒç»´ç ã€æ¡ç æ£€æµ‹
4ã€ç›®æ ‡è·Ÿè¸ª
5ã€æ–‡å­—æ£€æµ‹
6ã€äººè„¸æ£€æµ‹
7ã€äººè„¸é¢éƒ¨ç‰¹å¾æ£€æµ‹
ç”±äºå¯¹äººè„¸è¯†åˆ«æ¯”è¾ƒæ„Ÿå…´è¶£ï¼Œæ‰€ä»¥è¿™é‡Œå°±ä¸»è¦ç®€å•äº†è§£äº†ä¸‹äººè„¸éƒ¨åˆ†ï¼Œä¸‹é¢å°±é’ˆå¯¹äººè„¸æ£€æµ‹å’Œé¢éƒ¨æ£€æµ‹å†™å†™

#####Visionæ”¯æŒçš„å›¾ç‰‡ç±»å‹
é€šè¿‡æŸ¥çœ‹`VNRequestHandler.h`æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‡Œé¢çš„æ‰€æœ‰åˆå§‹åŒ–å‡½æ•°ï¼Œé€šè¿‡è¿™äº›åˆå§‹åŒ–å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£åˆ°æ”¯æŒçš„ç±»å‹æœ‰ï¼š
1ã€`CVPixelBufferRef`
2ã€`CGImageRef`
3ã€`CIImage`
4ã€`NSURL`
5ã€`NSData`

#####Visionä½¿ç”¨
åœ¨ä½¿ç”¨`vision`çš„æ—¶å€™ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ˜ç¡®è‡ªå·±éœ€è¦ä»€ä¹ˆæ•ˆæœï¼Œç„¶åæ ¹æ®æƒ³è¦çš„æ•ˆæœæ¥é€‰æ‹©ä¸åŒçš„ç±»ï¼Œæœ€åå®ç°è‡ªå·±çš„æ•ˆæœ
1ã€éœ€è¦ä¸€ä¸ª`RequestHandler`ï¼Œåœ¨åˆ›å»º`RequestHandler`çš„æ—¶å€™ï¼Œéœ€è¦ä¸€ä¸ªåˆé€‚çš„è¾“å…¥æºï¼ŒåŠ`å›¾ç‰‡`ç±»å‹
2ã€éœ€è¦ä¸€ä¸ª`Request `ï¼Œåœ¨åˆ›å»º`Request `çš„æ—¶å€™ï¼Œä¹Ÿéœ€è¦æ ¹æ®å®é™…æƒ…å†µæ¥é€‰æ‹©ï¼Œ`Request `å¤§æ¦‚æœ‰å¦‚ä¸‹è¿™ä¹ˆäº›

![request.jpeg](http://upload-images.jianshu.io/upload_images/2525768-cc3cd22263666ea1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
3ã€é€šè¿‡`requestHandler`å°†`request`è”ç³»èµ·æ¥ï¼Œç„¶åå¾—åˆ°ç»“æœ
```
[handler performRequests:@[requset] error:&error];
```
4ã€å¤„ç†ç»“æœ`VNObservation`ï¼Œåœ¨`VNRequest`çš„`results`æ•°ç»„ä¸­ï¼ŒåŒ…å«äº†`VNObservation`ç»“æœï¼Œ`VNObservation`ä¹Ÿåˆ†å¾ˆå¤šç§ï¼Œè¿™å’Œä½ `Request`çš„ç±»å‹æ˜¯ç›¸å…³è”çš„

![Visionç»“æœç»§æ‰¿å…³ç³».png](http://upload-images.jianshu.io/upload_images/2525768-deced22145c609ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

åœ¨å®Œæˆä¸Šè¿°æ­¥éª¤åï¼Œæˆ‘ä»¬å°±å¯ä»¥æ ¹æ®ç»“æœæ¥å®ç°ä¸€äº›æˆ‘ä»¬æƒ³è¦çš„æ•ˆæœ

#####äººè„¸çŸ©å½¢æ£€æµ‹
è¿™é‡Œæˆ‘ä»¬éœ€è¦ç”¨åˆ°`VNDetectFaceRectanglesRequest`
```
requset = [[VNDetectFaceRectanglesRequest alloc] initWithCompletionHandler:completionHandler];
```
åœ¨å¾—åˆ°ç»“æœåï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†ä¸‹åæ ‡
```
    for (VNFaceObservation *faceObservation in observations) {
        //boundingBox
        CGRect transFrame = [self convertRect:faceObservation.boundingBox imageSize:image.size];
        [rects addObject:[NSValue valueWithCGRect:transFrame]];
    }
```

```
// è½¬æ¢Rect
- (CGRect)convertRect:(CGRect)boundingBox imageSize:(CGSize)imageSize{
    CGFloat w = boundingBox.size.width * imageSize.width;
    CGFloat h = boundingBox.size.height * imageSize.height;
    CGFloat x = boundingBox.origin.x * imageSize.width;
    CGFloat y = imageSize.height * (1 - boundingBox.origin.y - boundingBox.size.height);//- (boundingBox.origin.y * imageSize.height) - h;
    return CGRectMake(x, y, w, h);
}
```
åœ¨è¿”å›ç»“æœä¸­çš„`boundingBox `ä¸­çš„åæ ‡ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ç«‹å³ä½¿ç”¨ï¼Œè€Œæ˜¯éœ€è¦è¿›è¡Œè½¬æ¢ï¼Œå› ä¸ºè¿™é‡Œæ˜¯ç›¸å¯¹äº`image`çš„ä¸€ä¸ªæ¯”ä¾‹ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯`y`åæ ‡çš„è½¬æ¢ï¼Œå› ä¸ºåæ ‡ç³»çš„`y`è½´å’Œ`UIView`çš„`y`è½´æ˜¯ç›¸åçš„ã€‚
æœ€åå°±æ˜¯é€šè¿‡è¿”å›çš„åæ ‡è¿›è¡ŒçŸ©å½¢çš„ç»˜åˆ¶
```
+ (UIImage *)gl_drawImage:(UIImage *)image withRects:(NSArray *)rects
{
    UIImage *newImage = nil;
    UIGraphicsBeginImageContextWithOptions(image.size, NO, [UIScreen mainScreen].scale);
    CGContextRef context = UIGraphicsGetCurrentContext();
    CGContextSetLineCap(context,kCGLineCapRound); //è¾¹ç¼˜æ ·å¼
    CGContextSetLineJoin(context, kCGLineJoinRound);
    CGContextSetLineWidth(context,2); //çº¿å®½
    CGContextSetAllowsAntialiasing(context,YES); //æ‰“å¼€æŠ—é”¯é½¿
    CGContextSetStrokeColorWithColor(context, [UIColor redColor].CGColor);
    CGContextSetFillColorWithColor(context, [UIColor clearColor].CGColor);
    
    //ç»˜åˆ¶å›¾ç‰‡
    [image drawInRect:CGRectMake(0, 0,image.size.width, image.size.height)];
    CGContextBeginPath(context);
    for (int i = 0; i < rects.count; i ++) {
        CGRect rect = [rects[i] CGRectValue];
        CGPoint sPoints[4];//åæ ‡ç‚¹
        sPoints[0] = CGPointMake(rect.origin.x, rect.origin.y);//åæ ‡1
        sPoints[1] = CGPointMake(rect.origin.x + rect.size.width, rect.origin.y);//åæ ‡2
        sPoints[2] = CGPointMake(rect.origin.x + rect.size.width, rect.origin.y + rect.size.height);//åæ ‡3
        sPoints[3] = CGPointMake(rect.origin.x , rect.origin.y + rect.size.height);
        
        CGContextAddLines(context, sPoints, 4);//æ·»åŠ çº¿
        CGContextClosePath(context); //å°é—­
    }
    CGContextDrawPath(context, kCGPathFillStroke); //æ ¹æ®åæ ‡ç»˜åˆ¶è·¯å¾„
    
    newImage = UIGraphicsGetImageFromCurrentImageContext();
    UIGraphicsEndImageContext();
    return newImage;
}
```
æ•ˆæœå¦‚ä¸‹


![faceRect.jpg](http://upload-images.jianshu.io/upload_images/2525768-a5febef321afbcaf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#####äººè„¸ç‰¹å¾è¯†åˆ«
è¿™é‡Œæˆ‘ä»¬éœ€è¦ç”¨åˆ°`VNDetectFaceLandmarksRequest`
```
            requset = [[VNDetectFaceLandmarksRequest alloc] initWithCompletionHandler:completionHandler];

```
å¤„ç†ç»“æœ
```
    for (VNFaceObservation *faceObservation in observations) {
        //boundingBox
        CGRect transFrame = [self convertRect:faceObservation.boundingBox imageSize:image.size];
        [rects addObject:[NSValue valueWithCGRect:transFrame]];
    }
    pointModel.faceRectPoints = rects;
    return pointModel;
}

- (GLDiscernPointModel *)handlerFaceLandMark:(NSArray *)observations image:(UIImage *)image
{
    GLDiscernPointModel *pointModel = [[GLDiscernPointModel alloc] init];
    NSMutableArray *rects = @[].mutableCopy;

    for (VNFaceObservation *faceObservation in observations) {
        
        VNFaceLandmarks2D *faceLandMarks2D = faceObservation.landmarks;
        
        [self getKeysWithClass:[VNFaceLandmarks2D class] block:^(NSString *key) {
            if ([key isEqualToString:@"allPoints"]) {
                return ;
            }
            VNFaceLandmarkRegion2D *faceLandMarkRegion2D = [faceLandMarks2D valueForKey:key];
            
            NSMutableArray *sPoints = [[NSMutableArray alloc] initWithCapacity:faceLandMarkRegion2D.pointCount];
            
            for (int i = 0; i < faceLandMarkRegion2D.pointCount; i ++) {
                CGPoint point = faceLandMarkRegion2D.normalizedPoints[i];
                
                CGFloat rectWidth = image.size.width * faceObservation.boundingBox.size.width;
                CGFloat rectHeight = image.size.height * faceObservation.boundingBox.size.height;
                CGPoint p = CGPointMake(point.x * rectWidth + faceObservation.boundingBox.origin.x * image.size.width, faceObservation.boundingBox.origin.y * image.size.height + point.y * rectHeight);
                [sPoints addObject:[NSValue valueWithCGPoint:p]];
            }
            
            [rects addObject:sPoints];
        }];
    }
```
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„åˆ°`landmarks `è¿™ä¸ªå±æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ª`VNFaceLandmarks2D`ç±»å‹çš„å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«ç€è®¸å¤šé¢éƒ¨ç‰¹å¾çš„`VNFaceLandmarkRegion2D`å¯¹è±¡ï¼Œå¦‚ï¼š`faceContour`ï¼Œ`leftEye`ï¼Œ`nose`....åˆ†åˆ«è¡¨ç¤ºé¢éƒ¨è½®å»“ã€å·¦çœ¼ã€é¼»å­ã€‚è¿™äº›å¯¹è±¡ä¸­ï¼ŒåˆåŒ…å«ä¸‹é¢è¿™ä¹ˆä¸€ä¸ªå±æ€§
```
@property (readonly, assign, nullable) const CGPoint* normalizedPoints
```
è¿™æ˜¯ä¸€ä¸ªåŒ…å«è¯¥é¢éƒ¨ç‰¹å¾çš„çš„æ•°ç»„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹å¼å–å‡ºé‡Œé¢çš„åæ ‡
```
CGPoint point = faceLandMarkRegion2D.normalizedPoints[i];
```
å½“ç„¶è¿™é‡Œé¢ä¹Ÿå­˜åœ¨åæ ‡çš„è½¬æ¢ï¼Œè§ä¸Šé¢ä»£ç 
æœ€åä¹Ÿæ˜¯ç”»çº¿ï¼Œä»£ç å¦‚ä¸‹
```
+ (UIImage *)gl_drawImage:(UIImage *)image faceLandMarkPoints:(NSArray *)landMarkPoints
{
    UIImage * newImage = image;
    for (NSMutableArray *points in landMarkPoints) {
        
        CGPoint sPoints [points.count];
        
        for (int i = 0;i <points.count;i++) {
            NSValue *pointValue = points[i];
            CGPoint point = pointValue.CGPointValue;
            sPoints[i] = point;
        }
        //ç”»çº¿
        UIGraphicsBeginImageContextWithOptions(newImage.size, NO, [UIScreen mainScreen].scale);
        CGContextRef context = UIGraphicsGetCurrentContext();
        CGContextSetLineCap(context,kCGLineCapRound); //è¾¹ç¼˜æ ·å¼
        CGContextSetLineJoin(context, kCGLineJoinRound);
        CGContextSetLineWidth(context,2); //çº¿å®½
        CGContextSetAllowsAntialiasing(context,YES); //æ‰“å¼€æŠ—é”¯é½¿
        // è®¾ç½®ç¿»è½¬
        CGContextTranslateCTM(context, 0, newImage.size.height);
        CGContextScaleCTM(context, 1.0, -1.0);
        
        CGContextSetStrokeColorWithColor(context, [UIColor redColor].CGColor);
        CGContextSetFillColorWithColor(context, [UIColor clearColor].CGColor);
        
        CGContextDrawImage(context, CGRectMake(0, 0,newImage.size.width,newImage.size.height), newImage.CGImage);
        
        CGContextBeginPath(context);
        CGContextAddLines(context, sPoints,points.count);//æ·»åŠ çº¿
        CGContextClosePath(context); //å°é—­
        CGContextDrawPath(context, kCGPathFillStroke); //æ ¹æ®åæ ‡ç»˜åˆ¶è·¯å¾„
        newImage = UIGraphicsGetImageFromCurrentImageContext();
        UIGraphicsEndImageContext();
    }
    return newImage;
}

```
æ•ˆæœå¦‚ä¸‹

![faceLandmark.png](http://upload-images.jianshu.io/upload_images/2525768-8560904c9eaf5f52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#####åŠ¨æ€äººè„¸çŸ©å½¢æ£€æµ‹
è¦åŠ¨æ€æ¥æ£€æµ‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬è‚¯å®šéœ€è¦é€šè¿‡ç›¸æœºæ¥å®æ—¶å–å‡ºèµ„æºï¼Œç„¶åå†å®ç°ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œé€‰æ‹©äº†`AVCapture`ï¼Œå…³äºç›¸æœºçš„åˆå§‹åŒ–åŠä½¿ç”¨æ–¹æ³•è¿™é‡Œå°±ä¸åœ¨ç´¯èµ˜äº†ï¼Œæˆ‘ä»¬ç›´æ¥ä¸Šä»£ç 
åœ¨`AVCaptureVideoDataOutputSampleBufferDelegate`ä¸­ï¼Œé€šè¿‡ä¸‹é¢çš„æ–¹æ³•
```
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection

```
æˆ‘ä»¬å¯ä»¥è¿›è¡Œè¿™ä¹ˆä¸€ä¸ªè½¬æ¢
```
CVPixelBufferRef cvpixeBufferRef = CMSampleBufferGetImageBuffer(sampleBuffer);
```
ç„¶åé€šè¿‡
```
    VNImageRequestHandler *handler = [[VNImageRequestHandler alloc] initWithCVPixelBuffer:cvpixeBufferRef options:@{}];

```
å°†ç›¸æœºè¿”å›çš„å›¾ç‰‡ä¸`request`è¿›è¡Œå…³è”äº†ã€‚
åç»­æ“ä½œå¦‚ä¸‹
```
            request = [[VNDetectFaceRectanglesRequest alloc] initWithCompletionHandler:^(VNRequest * _Nonnull request, NSError * _Nullable error) {
                
                NSLog(@" æ‰“å°ä¿¡æ¯:%lu",request.results.count);
                NSArray *vnobservations = request.results;
                
                dispatch_async(dispatch_get_main_queue(), ^{
                    //å…ˆç§»é™¤ä¹‹å‰çš„çŸ©å½¢æ¡†
                    [self.rectLayers makeObjectsPerformSelector:@selector(removeFromSuperlayer)];

                    AVCaptureDevicePosition position = [[self.avInput device] position];

                    
                    
                    for (VNFaceObservation *faceObservation in vnobservations) {
                        //boundingBox
                        CGRect transFrame = [[GLTools sharedInstance] convertRect:faceObservation.boundingBox imageSize:self.view.frame.size];
                        //å‰ç½®æ‘„åƒå¤´çš„æ—¶å€™ è®°å¾—è½¬æ¢
                        if (position == AVCaptureDevicePositionFront){
                            transFrame.origin.x = self.view.frame.size.width - transFrame.origin.x - transFrame.size.width;
                        }
                        
                        CALayer *rectLayer = [CALayer layer];
                        rectLayer.frame = transFrame;
                        rectLayer.borderColor = [UIColor purpleColor].CGColor;
                        rectLayer.borderWidth = 2;
                        [self.view.layer addSublayer:rectLayer];
                        
                        [self.rectLayers addObject:rectLayer];
                    }
                });
            }];
```
åœ¨è¿™é‡Œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯æ‘„åƒå¤´åˆ†ä¸ºå‰åæ‘„åƒå¤´ï¼Œæ‰€ä»¥åœ¨å‰ç½®æ‘„åƒå¤´å’Œåç½®æ‘„åƒå¤´åˆ‡æ¢çš„æ—¶å€™ï¼Œéœ€è¦é‡æ–°é…ç½®ä¸‹
```
            //éœ€è¦é‡æ–°è¿›è¡Œé…ç½®è¾“å‡º ç‰¹åˆ«æ˜¯ä¸‹é¢çš„è¾“å‡ºæ–¹å‘
            AVCaptureConnection *captureConnection = [self.avOutput connectionWithMediaType:AVMediaTypeVideo];
            if ([captureConnection isVideoOrientationSupported]) {
                [captureConnection setVideoOrientation:AVCaptureVideoOrientationPortrait];
            }
            // è§†é¢‘ç¨³å®šè®¾ç½®
            if ([captureConnection isVideoStabilizationSupported]) {
                captureConnection.preferredVideoStabilizationMode = AVCaptureVideoStabilizationModeAuto;
            }
            // è®¾ç½®è¾“å‡ºå›¾ç‰‡æ–¹å‘
            captureConnection.videoOrientation = AVCaptureVideoOrientationPortrait;
```
è¿˜æœ‰ä¸ªé—®é¢˜å°±æ˜¯åœ¨åæ ‡è½¬åŒ–çš„æ—¶å€™ï¼Œå‰ç½®æ‘„åƒå¤´çš„`x`è½´å’Œ`UIView`çš„`x`è½´ä¹Ÿæ˜¯ç›¸åçš„ï¼Œæ‰€ä»¥è¿™é‡Œä¹Ÿéœ€è¦åœ¨è¿›è¡Œä¸€æ¬¡è½¬åŒ–
```
 transFrame.origin.x = self.view.frame.size.width - transFrame.origin.x - transFrame.size.width;

```
æ•ˆæœå¦‚ä¸‹


![åŠ¨æ€1.gif](http://upload-images.jianshu.io/upload_images/2525768-66b667e7e5d9a9e5.gif?imageMogr2/auto-orient/strip)

#####åŠ¨æ€æ·»åŠ åœºæ™¯
å…³äºåŠ¨æ€æ·»åŠ åœºæ™¯ï¼Œå…¶å®å°±åƒæˆ‘ä»¬å¹³æ—¶ç”¨çš„ç¾é¢œç›¸æœºé‚£æ ·ï¼Œåœ¨é€‚å½“çš„ä½ç½®æ·»åŠ äº›å¸½å­ã€çœ¼é•œç­‰å„ç§æç¬‘çš„å›¾ç‰‡ã€‚è¿™é‡Œæˆ‘ä»¬è¿˜æ˜¯éœ€è¦ç”¨åˆ°`AVCapture `ï¼Œå¹¶ä¸”å’ŒåŠ¨æ€æ·»åŠ çŸ©å½¢çš„æ–¹æ³•ç±»ä¼¼ï¼Œåªæ˜¯åœ¨`request`ä¸Šå’Œå¤„ç†æ–¹å¼ä¸Šä¸ä¸€æ ·
ä¸‹é¢æˆ‘ä»¬å…ˆçœ‹ä»£ç 
```
            request = [[VNDetectFaceLandmarksRequest alloc] initWithCompletionHandler:^(VNRequest * _Nonnull request, NSError * _Nullable error) {
                NSArray *vnobservations = request.results;
                
                
                for (VNFaceObservation *faceObservation in vnobservations) {
                    
                    
                    VNFaceLandmarks2D *faceLandMarks2D = faceObservation.landmarks;
                    
                    VNFaceLandmarkRegion2D *leftEyefaceLandMarkRegion2D = faceLandMarks2D.leftEye;
                    VNFaceLandmarkRegion2D *rightEyefaceLandMarkRegion2D = faceLandMarks2D.rightEye;
                    
                    dispatch_async(dispatch_get_main_queue(), ^{
                        
//                        //å…ˆç§»é™¤ä¹‹å‰çš„çŸ©å½¢æ¡†
//                        [self.rectLayers makeObjectsPerformSelector:@selector(removeFromSuperlayer)];
//
//                        AVCaptureDevicePosition position = [[self.avInput device] position];
//
//                        CGRect transFrame = [[GLTools sharedInstance] convertRect:faceObservation.boundingBox imageSize:self.view.frame.size];
//                        //å‰ç½®æ‘„åƒå¤´çš„æ—¶å€™ è®°å¾—è½¬æ¢
//                        if (position == AVCaptureDevicePositionFront){
//                            transFrame.origin.x = self.view.frame.size.width - transFrame.origin.x - transFrame.size.width;
//                        }
//
//                        CALayer *rectLayer = [CALayer layer];
//                        rectLayer.frame = transFrame;
//                        rectLayer.borderColor = [UIColor purpleColor].CGColor;
//                        rectLayer.borderWidth = 2;
//                        [self.view.layer addSublayer:rectLayer];
//
//                        [self.rectLayers addObject:rectLayer];
                        
                        AVCaptureDevicePosition position = [[self.avInput device] position];

                        
                        CGPoint sPoints[leftEyefaceLandMarkRegion2D.pointCount + rightEyefaceLandMarkRegion2D.pointCount];
                        
                        NSMutableArray *pointXs = [[NSMutableArray alloc] init];
                        NSMutableArray *pointYs = [[NSMutableArray alloc] init];
                        
                        for (int i = 0; i < leftEyefaceLandMarkRegion2D.pointCount; i ++) {
                            CGPoint point = leftEyefaceLandMarkRegion2D.normalizedPoints[i];
                            
                            CGFloat rectWidth = self.view.bounds.size.width * faceObservation.boundingBox.size.width;
                            CGFloat rectHeight = self.view.bounds.size.height * faceObservation.boundingBox.size.height;
                            
                            CGFloat boundingBoxY = self.view.bounds.size.height * (1 - faceObservation.boundingBox.origin.y - faceObservation.boundingBox.size.height);
                            
                            CGPoint p = CGPointZero;
                            if (position == AVCaptureDevicePositionFront){
                                
                                CGFloat boundingX = self.view.frame.size.width - faceObservation.boundingBox.origin.x * self.view.bounds.size.width - rectWidth;
                                
                                p = CGPointMake(point.x * rectWidth + boundingX, boundingBoxY + (1-point.y) * rectHeight);

                            }else{
                              p = CGPointMake(point.x * rectWidth + faceObservation.boundingBox.origin.x * self.view.bounds.size.width, boundingBoxY + (1-point.y) * rectHeight);
                            }

                            sPoints[i] = p;
                            
                            [pointXs addObject:[NSNumber numberWithFloat:p.x]];
                            [pointYs addObject:[NSNumber numberWithFloat:p.y]];
                        }
                        
                        for (int j = 0; j < rightEyefaceLandMarkRegion2D.pointCount; j ++) {
                            CGPoint point = rightEyefaceLandMarkRegion2D.normalizedPoints[j];

                            CGFloat rectWidth = self.view.bounds.size.width * faceObservation.boundingBox.size.width;
                            CGFloat rectHeight = self.view.bounds.size.height * faceObservation.boundingBox.size.height;

                            CGFloat boundingBoxY = self.view.bounds.size.height * (1 - faceObservation.boundingBox.origin.y - faceObservation.boundingBox.size.height);

                            CGPoint p = CGPointZero;
                            if (position == AVCaptureDevicePositionFront){
                                
                                CGFloat boundingX = self.view.frame.size.width - faceObservation.boundingBox.origin.x * self.view.bounds.size.width - rectWidth;
                                
                                p = CGPointMake(point.x * rectWidth + boundingX, boundingBoxY + (1-point.y) * rectHeight);
                                
                            }else{
                                p = CGPointMake(point.x * rectWidth + faceObservation.boundingBox.origin.x * self.view.bounds.size.width, boundingBoxY + (1-point.y) * rectHeight);
                            }
                            
                            sPoints[leftEyefaceLandMarkRegion2D.pointCount + j] = p;
                            
                            [pointXs addObject:[NSNumber numberWithFloat:p.x]];
                            [pointYs addObject:[NSNumber numberWithFloat:p.y]];
                        }
                        
//                        for (UIView *view in self.view.subviews) {
//                            if ([view isKindOfClass:[UIImageView class]]) {
//                                [view removeFromSuperview];
//                            }
//                        }
//
//                        for (int i = 0; i < rightEyefaceLandMarkRegion2D.pointCount + leftEyefaceLandMarkRegion2D.pointCount; i++) {
//                            CGFloat x = sPoints[i].x;
//                            CGFloat y = sPoints[i].y;
//                            UIImageView *view = [[UIImageView alloc] initWithFrame:CGRectMake(x, y, 2, 2)];
//                            view.backgroundColor = [UIColor redColor];
//                            [self.view addSubview:view];
//                        }
                        
                        //æ’åº å¾—åˆ°æœ€å°çš„xå’Œæœ€å¤§çš„x
                        NSArray *sortPointXs = [pointXs sortedArrayWithOptions:NSSortStable usingComparator:
                                                ^NSComparisonResult(id  _Nonnull obj1, id  _Nonnull obj2) {
                                                    int value1 = [obj1 floatValue];
                                                    int value2 = [obj2 floatValue];
                                                    if (value1 > value2) {
                                                        return NSOrderedDescending;
                                                    }else if (value1 == value2){
                                                        return NSOrderedSame;
                                                    }else{
                                                        return NSOrderedAscending;
                                                    }
                                                }];

                        NSArray *sortPointYs = [pointYs sortedArrayWithOptions:NSSortStable usingComparator:
                                                ^NSComparisonResult(id  _Nonnull obj1, id  _Nonnull obj2) {
                                                    int value1 = [obj1 floatValue];
                                                    int value2 = [obj2 floatValue];
                                                    if (value1 > value2) {
                                                        return NSOrderedDescending;
                                                    }else if (value1 == value2){
                                                        return NSOrderedSame;
                                                    }else{
                                                        return NSOrderedAscending;
                                                    }
                                                }];
                        
                        UIImage *image =[UIImage imageNamed:@"eyes"];
                        CGFloat imageWidth = [sortPointXs.lastObject floatValue] - [sortPointXs.firstObject floatValue] + 40;
                        CGFloat imageHeight = (imageWidth * image.size.height)/image.size.width;
                        
                        self.glassesImageView.frame = CGRectMake([sortPointXs.firstObject floatValue]-20, [sortPointYs.firstObject floatValue]-5, imageWidth, imageHeight);
                    });
                }
            }];
```
ç”±äºæ—¶é—´å…³ç³»ï¼Œä»£ç æœ‰ç‚¹ä¹±ï¼Œå°†å°±å°†å°±

å…ˆè¯´è¯´æ€è·¯ï¼Œæˆ‘æ˜¯æƒ³åŠ¨æ€æ·»åŠ ä¸€ä¸ªçœ¼é•œçš„ï¼Œæ‰€ä»¥æˆ‘å¿…é¡»å…ˆå¾—åˆ°ä¸¤ä¸ªçœ¼ç›çš„ä½ç½®ï¼Œç„¶ååœ¨è®¡ç®—å‡ºä¸¤ä¸ªçœ¼ç›çš„å®½é«˜ï¼Œæœ€åé€‚å½“çš„è°ƒæ•´çœ¼é•œçš„å¤§å°ï¼Œå†åŠ¨æ€çš„æ·»åŠ ä¸Šå»

è¿™é‡Œå¿…é¡»è¦è¯´çš„ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯æˆ‘åœ¨å®ç°è¿‡ç¨‹ä¸­é‡åˆ°çš„---`åæ ‡`

é¦–å…ˆæ˜¯`y`åæ ‡ï¼Œå¦‚æœè¿˜æ˜¯æŒ‰ç…§é™æ€å›¾ç‰‡çš„é‚£ç§è·å–æ–¹å¼ï¼Œé‚£ä¹ˆå¾—åˆ°çš„ç»“æœå°†ä¼šæ˜¯å®Œå…¨ç›¸åçš„ã€‚
```
faceObservation.boundingBox.origin.y * image.size.height + point.y * rectHeight
```
è¿™é‡Œæˆ‘åšäº† ä¸€ä¸ªå‡è®¾ï¼Œä¼°è®¡æ˜¯ç”±äºæ‘„åƒæœºæˆåƒçš„åŸå› é€ æˆçš„ï¼Œæ‰€ä»¥å¿…é¡»åå…¶é“è€Œè¡Œï¼Œäºæ˜¯æˆ‘å¦‚ä¸‹æ”¹é€ äº†ä¸‹
```
 CGFloat boundingBoxY = self.view.bounds.size.height * (1 - faceObservation.boundingBox.origin.y - faceObservation.boundingBox.size.height);

 p = CGPointMake(point.x * rectWidth + faceObservation.boundingBox.origin.x * self.view.bounds.size.width, boundingBoxY + (1-point.y) * rectHeight);
```
ä»ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ‰€æœ‰çš„`point.y`éƒ½ç”¨`1`å‡å»äº†ï¼Œè¿™ä¸ªè¯•éªŒçš„è¿‡ç¨‹æœ‰ç‚¹æ¼ç«ï¼Œæˆ‘è¿˜æ²¡æ€ä¹ˆç›¸é€šï¼Œè‹¥æœ‰çŸ¥é“çš„ï¼Œå¸Œæœ›å¯ä»¥å‘Šè¯‰æˆ‘ä¸‹ï¼Œå½“ç„¶æˆ‘ä¹Ÿä¼šå†ç ”ç©¶ç ”ç©¶ã€‚
å†è¯´å®Œ`y`åæ ‡åï¼Œå°±æ˜¯`x`åæ ‡äº†ï¼Œ`x`åæ ‡åœ¨`å‰ç½®æ‘„åƒå¤´`çš„æ—¶å€™ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶è€Œåœ¨åˆ‡æ¢æˆ`åç½®æ‘„åƒå¤´`çš„æ—¶å€™ï¼Œåˆåäº†ã€‚ğŸ˜”ï¼å¿ƒç´¯å•Šï¼Œæ‰€ä»¥æ²¡åŠæ³•ï¼Œæˆ‘å°±åªè¦åŠ åˆ¤æ–­ï¼Œç„¶åè¿›è¡Œæµ‹è¯•ï¼Œæœ‰äº†å¦‚ä¸‹ä»£ç 
```
 CGFloat boundingX = self.view.frame.size.width - faceObservation.boundingBox.origin.x * self.view.bounds.size.width - rectWidth;
```
æœ€åç»ˆäºå¤§åŠŸå‘Šæˆï¼
æ•ˆæœå°±æ˜¯æ–‡ç« æœ€é¡¶çš„é‚£ä¸ªæ•ˆæœ

#####æ³¨æ„
1ã€åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°å½“æ£€æµ‹å›¾ç‰‡çš„æ—¶å€™å†…å­˜å’Œ`cpu`çš„æ¶ˆè€—è¿˜æ˜¯å¾ˆé«˜çš„ï¼Œæ¯”å¦‚æˆ‘çš„`5s`å°±æˆåŠŸçš„å´©æºƒè¿‡.....
2ã€å›¾ç‰‡æ–¹å‘æ˜¯æœ‰è¦æ±‚çš„....
```
- (instancetype)initWithCVPixelBuffer:(CVPixelBufferRef)pixelBuffer options:(NSDictionary<VNImageOption, id> *)options;

/*!
 @brief initWithCVPixelBuffer:options creates a VNImageRequestHandler to be used for performing requests against the image passed in as buffer.
 
 @param pixelBuffer A CVPixelBuffer containing the image to be used for performing the requests. The content of the buffer cannot be modified for the lifetime of the VNImageRequestHandler.
 @param orientation The orientation of the image/buffer based on the EXIF specification. For details see kCGImagePropertyOrientation. The value has to be an integer from 1 to 8. This superceeds every other orientation information.
 @param options A dictionary with options specifying auxilary information for the buffer/image like VNImageOptionCameraIntrinsics
 */
- (instancetype)initWithCVPixelBuffer:(CVPixelBufferRef)pixelBuffer orientation:(CGImagePropertyOrientation)orientation options:(NSDictionary<VNImageOption, id> *)options;

```
é€šè¿‡å¯¹æ¯”ä¸Šé¢ä¸¤ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå¤šäº†ä¸€ä¸ª`CGImagePropertyOrientation `ç±»å‹çš„å‚æ•°ï¼Œæ²¡é”™ï¼Œè¿™å°±æ˜¯æŒ‡å®šä¼ å…¥å›¾ç‰‡çš„æ–¹å‘ï¼Œå¦‚æœæŒ‡å®šäº†æ–¹å‘ï¼Œè€Œå›¾ç‰‡æ–¹å‘å´ä¸ä¸€è‡´ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼Œæ£€æµ‹ä¸å‡ºæ¥....è¿™é‡Œæˆ‘ç”¨çš„éƒ½æ˜¯ç¬¬ä¸€ä¸ªæ–¹æ³•ï¼ŒåŠæ²¡æœ‰å‚æ•°ï¼Œå¥½åƒé»˜è®¤æ˜¯`up`çš„ã€‚

#####æœ€å
è¿˜æ˜¯é™„ä¸Š[Demo](https://github.com/gao211326/VisionDemo)ï¼Œå¦‚æœè§‰å¾—è¿˜è¡Œçš„è¯ï¼Œæ¬¢è¿å¤§å®¶ç»™ä¸ª`star`ï¼æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œå¯ä»¥å¤šå¤šæ²Ÿé€š
